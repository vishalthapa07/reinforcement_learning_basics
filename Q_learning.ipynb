{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4666b0-f41a-434a-abb5-bdfe46340058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "#define the grid world\n",
    "GRID_SIZE = 3\n",
    "START = (0,0)\n",
    "GOAL = (2,2)\n",
    "OBSTACLE = (1,1)\n",
    "\n",
    "#define actions\n",
    "ACTIONS = [\n",
    "    (-1, 0), #up\n",
    "    (1, 0), #down\n",
    "    (0, 1), #right\n",
    "    (0, -1), #left\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df22ee-d596-469c-99b6-91f109a68651",
   "metadata": {},
   "source": [
    "is_valid_state ensures state is within grid boundaries, returns True if valid, False if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38737781-24e2-47d1-bf8e-efcec4ec0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
    "    return (0 <= state[0] < GRID_SIZE and\n",
    "            0 <= state [1] < GRID_SIZE and\n",
    "            state != OBSTACLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ca405-b604-4bf7-9bf9-2fdc616959cb",
   "metadata": {},
   "source": [
    "get_next_state adds the action taken to the current state and checks if it's a valid\n",
    "- if valid, returns new position\n",
    "- if invalid, returns same position (agent doesn't move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eebe3d0-e9be-4c93-8edc-b8b10efecf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state: Tuple[int, int], action: Tuple[int, int]) -> Tuple [int, int]:\n",
    "    next_state = (state[0] + action[0], state [1] + action[1])\n",
    "    return next_state if is_valid_state(next_state) else state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0f868-fe66-464f-8830-8c4adeda1e15",
   "metadata": {},
   "source": [
    "## Defining Q-Learning paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85808457-1458-46b1-a49f-b355ecc995b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.3\n",
    "ALPHA = 0.3\n",
    "GAMMA = 0.99\n",
    "EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b66f57-c218-4b9a-8d08-8f2b87b3f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state: Tuple[int, int], next_state: Tuple[int, int]) -> int:\n",
    "    if next_state == GOAL:\n",
    "        return 100\n",
    "    elif next_state == OBSTACLE or next_state == state: #penelize hitting walls or obstacle\n",
    "        return -10\n",
    "    else:\n",
    "        return -1 #small penalty for each step to encourage shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0306c208-f8fe-4605-9dcc-00d35c525b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state: Tuple[int, int], q_table: np.ndarray) -> Tuple[int, int]:\n",
    "    if random.uniform(0,1) < EPSILON:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(q_table[state])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c031750a-7610-4531-98de-45c6a88f1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table: np.ndarray, state: Tuple[int, int], action: Tuple[int, int], reward: int, next_state: Tuple[int, int]) -> None:\n",
    "    action_idx = ACTIONS.index(action)\n",
    "    q_table[state][action_idx] += ALPHA * (reward + GAMMA * np.max(q_table[next_state]) - q_table[state][action_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15372c7-a128-4a10-bb82-44f8062ffa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent() -> np.ndarray:\n",
    "    q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "    for _ in range(EPISODES):\n",
    "        state = START\n",
    "        while state != GOAL:\n",
    "            action = choose_action(state, q_table)\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = get_reward(state, next_state)\n",
    "            update_q_table(q_table, state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "    return q_table\n",
    "\n",
    "# train the agent\n",
    "q_table = train_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7159c7-cb85-45d6-a440-c3b7cc0149e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table_as_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the Q-table as a grid with all action values for each state.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nDetailed Q-table Grid:\")\n",
    "    \n",
    "    # Header\n",
    "    header = \"   |\" + \"|\".join(f\"   ({i},{j})   \" for i in range(GRID_SIZE) for j in range(GRID_SIZE)) + \"|\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for action_idx, action_symbol in enumerate(action_symbols):\n",
    "        row = f\" {action_symbol} |\"\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if (i, j) == GOAL:\n",
    "                    cell = \"   GOAL    \"\n",
    "                elif (i, j) == OBSTACLE:\n",
    "                    cell = \" OBSTACLE  \"\n",
    "                else:\n",
    "                    q_value = q_table[i, j, action_idx]\n",
    "                    cell = f\" {q_value:9.2f} \"\n",
    "                row += cell + \"|\"\n",
    "        print(row)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "def visualize_best_actions_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the best action and its Q-value for each state in a grid.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nBest Actions Grid:\")\n",
    "    header = \"-\" * (14 * GRID_SIZE + 1)\n",
    "    print(header)\n",
    "\n",
    "    for i in range(GRID_SIZE):\n",
    "        row = \"| \"\n",
    "        for j in range(GRID_SIZE):\n",
    "            if (i, j) == GOAL:\n",
    "                cell = \"   GOAL    \"\n",
    "            elif (i, j) == OBSTACLE:\n",
    "                cell = \" OBSTACLE  \"\n",
    "            else:\n",
    "                best_action_idx = np.argmax(q_table[i, j])\n",
    "                best_q_value = q_table[i, j, best_action_idx]\n",
    "                cell = f\"{action_symbols[best_action_idx]}:{best_q_value:7.2f}  \"\n",
    "            row += cell + \" | \"\n",
    "        print(row)\n",
    "        print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f17e863-891a-4be1-8a0f-242beb7f4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Q-table Grid:\n",
      "   |   (0,0)   |   (0,1)   |   (0,2)   |   (1,0)   |   (1,1)   |   (1,2)   |   (2,0)   |   (2,1)   |   (2,2)   |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " ^ |     83.12 |     85.06 |     87.02 |     92.12 | OBSTACLE  |     96.02 |     94.06 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " > |     94.06 |     85.06 |     98.00 |     96.02 | OBSTACLE  |    100.00 |     87.02 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " v |     94.06 |     96.02 |     87.02 |     85.06 | OBSTACLE  |     89.00 |     98.00 |    100.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " < |     83.12 |     92.12 |     94.06 |     85.06 | OBSTACLE  |     89.00 |     87.02 |     96.02 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best Actions Grid:\n",
      "-------------------------------------------\n",
      "| >:  94.06   | v:  96.02   | >:  98.00   | \n",
      "-------------------------------------------\n",
      "| >:  96.02   |  OBSTACLE   | >: 100.00   | \n",
      "-------------------------------------------\n",
      "| v:  98.00   | v: 100.00   |    GOAL     | \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#visualize the Q-table as a grid\n",
    "visualize_q_table_as_grid(q_table)\n",
    "\n",
    "#visualize the best actions and their Q-values in a grid\n",
    "visualize_best_actions_grid(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b5f5c-d472-40eb-83d9-f6b51e60544a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
